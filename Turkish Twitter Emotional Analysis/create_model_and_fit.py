# -*- coding: utf-8 -*-
"""create_model_and_fit

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dD9W6CxDle2n9hAGhs-yyNHhDs8OUfv1
"""

from keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Flatten, MaxPooling2D, Activation, SpatialDropout1D
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from keras.utils.vis_utils import plot_model
from keras.layers.recurrent import LSTM
from keras.optimizers import SGD
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from numpy import asarray
from numpy import zeros
import tensorflow as tf
import numpy as np

#Load Dictionary
vocab_path = '/content/drive/MyDrive/EmotionAnalysis/emotional_vocab.txt'

# load doc into memory
def load_doc(filename):
  # open the file as read only
  file = open(filename, 'r', encoding='utf-8')
  # read all text
  text = file.read()
  # close the file
  file.close()
  return text

# load the vocabulary
vocab_filename = vocab_path
vocab = load_doc(vocab_filename)
print(len(vocab))
vocab = set(vocab.split())
print(len(vocab))

# Create Model and data preprocessing
def clean_doc(doc):
  # split into tokens by white space
  tokens = doc.split()
  tokens = [word.lower() for word in tokens]  
  # prepare regex for char filtering
  re_punc = re.compile('[%s]' % re.escape(string.punctuation))
  # remove punctuation from each word
  tokens = [re_punc.sub('', w) for w in tokens]
  # filter out short tokens
  tokens = [turkStem.stemWord(word) for word in tokens]
  tokens = [w for w in tokens if not w in tr_stopwords]
  tokens = [w for w in tokens if w in vocab]
  tokens = [word for word in tokens if len(word) > 1]
  return " ".join(tokens)

def create_tokenizer(lines):  
  tokenizer = Tokenizer()
  tokenizer.fit_on_texts(lines)
  return tokenizer

def encode_docs(tokenizer, max_length, docs):
  # integer encode
  encoded = tokenizer.texts_to_sequences(docs)
  # pad sequences
  padded = pad_sequences(encoded, maxlen=max_length, padding='post')
  return padded

def define_cnn_model(vocab_size, max_length):
  model = Sequential()
  model.add(Embedding(vocab_size, 8, input_length=max_length)) # Same as the books paramethers
  model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))
  model.add(Dropout(0.2))
  model.add(MaxPooling1D(pool_size=2))
  model.add(Flatten()) 
  model.add(Dense(128, activation='relu'))
  model.add(Dense(6, activation='softmax'))
  # compile network
  model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])
  # summarize defined model
  model.summary()
  plot_model(model, to_file='modelexp1.png', show_shapes=True)
  return model

def define_rnn_model(vocab_size, max_length):
  model1 = Sequential()
  model1.add(Embedding(vocab_size, 100, input_length=max_length))
  model1.add(Dropout(0.2))
  model1.add(LSTM(128, activation='relu'  ))
  model1.add(Dense(128, activation='relu'))
  model1.add(Dense(6, activation='sigmoid'))
  # compile network
  model1.compile(loss='categorical_crossentropy', optimizer= 'adamax', metrics=['accuracy'])
  # summarize layers
  model1.summary()
  # plot graph
  plot_model(model1, to_file='recurrent_neural_network.png')
  return model1

def define_ann_model(n_words): 
  model = Sequential()
  model.add(Dense(1000, input_shape=(n_words,), activation='relu'))
  model.add(Dropout(0.2))
  model.add(Dense(128, activation='relu')) 
  model.add(Dense(6, activation='softmax'))
  model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])
  # summarize layers
  model.summary()
  # plot graph
  plot_model(model, to_file='artificial_neural_network.png')
  return model

clean_corpus = []

for line in corpus_:
  line = re.sub(r'http\S+', '', line)
  line = re.sub(r'@\S+', '', line)
  line = deEmojify(line)
  line = clean_doc(line)
  line = [w for w in line if w in vocab] 
  clean_corpus.append(line)

X_train, X_test, y_train, y_test = train_test_split(clean_corpus, emotions_, test_size = 0.1, random_state = 42)

train_docs = X_train
test_docs = X_test

#Fit model

# create the tokenizer
tokenizer = create_tokenizer(train_docs)
# define vocabulary size
vocab_size = len(tokenizer.word_index) + 1
print('Vocabulary size: %d' % vocab_size)
# calculate the maximum sequence length
max_length = max([len(s.split()) for s in train_docs])

print('Maximum length: %d' % max_length)

# encode data
Xtrain = encode_docs(tokenizer, max_length, X_train)
#ytrain = encode_docs(tokenizer_emotions, 6, train_emo)
Xtest = encode_docs(tokenizer, max_length, X_test)
#encode emotions
ytrain=pd.get_dummies(y_train)
ytest=pd.get_dummies(y_test)

# define model
#model = define_ann_model(Xtrain.shape[1])
#model = define_rnn_model(vocab_size, max_length)
model = define_cnn_model(vocab_size, max_length)

# fit network
model.fit(Xtrain, ytrain, epochs=10, verbose=2) #Xtrain yerine X_train koydum
model.save('modelv7.h5')