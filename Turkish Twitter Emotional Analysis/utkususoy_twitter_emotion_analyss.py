# -*- coding: utf-8 -*-
"""utkususoy_twitter_emotion_analyss.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W91r7UCsTCIxW24X5SwMhOuCp0kapjjU
"""

#Create Dictionary
import string
import re
from os import listdir
from numpy import array
from nltk.tokenize import word_tokenize
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.vis_utils import plot_model
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Embedding
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
import csv
from collections import defaultdict
import pandas as pd
from bs4 import BeautifulSoup
from collections import Counter
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
import nltk 
nltk.download('punkt')
from snowballstemmer import TurkishStemmer

turkStem=TurkishStemmer()

def deEmojify(text):
    regrex_pattern = re.compile("["
                u"\U0001F600-\U0001F64F"  # emoticons
                u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                u"\U0001F680-\U0001F6FF"  # transport & map symbols
                u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                u"\U00002702-\U000027B0"
                u"\U000024C2-\U0001F251"
                u"\U0001f926-\U0001f937"
                u'\U00010000-\U0010ffff'
                u"\u200d"
                u"\u2640-\u2642"
                u"\u2600-\u2B55"
                u"\u23cf"
                u"\u23e9"
                u"\u231a"
                u"\u3030"
                u"\ufe0f"
    "]+", flags=re.UNICODE)

    return regrex_pattern.sub(r'',text)

# turn a doc into clean tokens
def clean_doc(doc, tr_stopwords):
  # split into tokens by white space
  tokens = doc.split()
  tokens = [word.lower() for word in tokens] 
  # prepare regex for char filtering
  re_punc = re.compile('[%s]' % re.escape(string.punctuation))
  # remove punctuation from each word
  tokens = [re_punc.sub('', w) for w in tokens]
  # filter out short tokens
  tokens = [turkStem.stemWord(word) for word in tokens]
  tokens = [w for w in tokens if not w in tr_stopwords]
  tokens = [word for word in tokens if len(word) > 1]
  return tokens

def save_list(lines, filename):
  # convert lines to a single blob of text
  data = '\n'.join(lines)
  # open file
  file = open(filename, 'w', encoding='utf-8')
  # write text
  file.write(data)
  # close file
  file.close()

def load_doc_pandas(filename): 
  df = pd.read_excel(filename) # can also index sheet by name or fetch all sheets
  corpus = df['tw_text'].tolist()
  emotions = df['emotion'].tolist()
  id = df['id'].tolist()
  return corpus, emotions, id

def load_file(filename):
  # open the file as read only
  file = open(filename, 'r')
  # read all text
  text = file.read()
  # close the file
  file.close()
  text = word_tokenize(text)
  return text

def stem_stopwords(tr_stopwords):
  #stem's of each word 
  print("cccc",tr_stopwords[0])
  stemed_stopwords = [] 
  for w in tr_stopwords:  
    x = turkStem.stemWord(w) 
    stemed_stopwords.append(x) 
  return stemed_stopwords

stopwords_file = '/content/gdrive/MyDrive/EmotionAnalysis/stopwords_extend.txt' 
tr_stopwords = load_file(stopwords_file)

vocab = Counter()
filename = '/content/gdrive/MyDrive/EmotionAnalysis/Corpurav2.xlsx'

corpus_, emotions_, id_ = load_doc_pandas(filename)

#clean corpura and create vocab
for line in corpus_:
  line = re.sub(r'http\S+', '', line)
  line = re.sub(r'@\S+', '', line)
  line = deEmojify(line)
  line = clean_doc(line, tr_stopwords)
  vocab.update(line)

print(len(vocab))
print(vocab.most_common(50))

min_occurane = 2
final_vocab = [k for k,c in vocab.items() if c >= min_occurane]
print(len(final_vocab))

# save tokens to a vocabulary file
save_list(final_vocab, 'emotional_vocab.txt')

#Load Dictionary
vocab_path = '/content/drive/MyDrive/EmotionAnalysis/emotional_vocab.txt'

# load doc into memory
def load_doc(filename):
  # open the file as read only
  file = open(filename, 'r', encoding='utf-8')
  # read all text
  text = file.read()
  # close the file
  file.close()
  return text

# load the vocabulary
vocab_filename = vocab_path
vocab = load_doc(vocab_filename)
print(len(vocab))
vocab = set(vocab.split())
print(len(vocab))

# Create Model and data preprocessing
from keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Flatten, MaxPooling2D, Activation, SpatialDropout1D
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from keras.utils.vis_utils import plot_model
from keras.layers.recurrent import LSTM
from keras.optimizers import SGD
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from numpy import asarray
from numpy import zeros
import tensorflow as tf
import numpy as np

def clean_doc(doc):
  # split into tokens by white space
  tokens = doc.split()
  tokens = [word.lower() for word in tokens]  
  # prepare regex for char filtering
  re_punc = re.compile('[%s]' % re.escape(string.punctuation))
  # remove punctuation from each word
  tokens = [re_punc.sub('', w) for w in tokens]
  # filter out short tokens
  tokens = [turkStem.stemWord(word) for word in tokens]
  tokens = [w for w in tokens if not w in tr_stopwords]
  tokens = [w for w in tokens if w in vocab]
  tokens = [word for word in tokens if len(word) > 1]
  return " ".join(tokens)

def create_tokenizer(lines):  
  tokenizer = Tokenizer()
  tokenizer.fit_on_texts(lines)
  return tokenizer

def encode_docs(tokenizer, max_length, docs):
  # integer encode
  encoded = tokenizer.texts_to_sequences(docs)
  # pad sequences
  padded = pad_sequences(encoded, maxlen=max_length, padding='post')
  return padded

def define_cnn_model(vocab_size, max_length):
  model = Sequential()
  model.add(Embedding(vocab_size, 8, input_length=max_length)) # Same as the books paramethers
  model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))
  model.add(Dropout(0.2))
  model.add(MaxPooling1D(pool_size=2))
  model.add(Flatten()) 
  model.add(Dense(128, activation='relu'))
  model.add(Dense(6, activation='softmax'))
  # compile network
  model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])
  # summarize defined model
  model.summary()
  plot_model(model, to_file='modelexp1.png', show_shapes=True)
  return model

def define_rnn_model(vocab_size, max_length):
  model1 = Sequential()
  model1.add(Embedding(vocab_size, 100, input_length=max_length))
  model1.add(Dropout(0.2))
  model1.add(LSTM(128, activation='relu'  ))
  model1.add(Dense(128, activation='relu'))
  model1.add(Dense(6, activation='sigmoid'))
  # compile network
  model1.compile(loss='categorical_crossentropy', optimizer= 'adamax', metrics=['accuracy'])
  # summarize layers
  model1.summary()
  # plot graph
  plot_model(model1, to_file='recurrent_neural_network.png')
  return model1

def define_ann_model(n_words): 
  model = Sequential()
  model.add(Dense(1000, input_shape=(n_words,), activation='relu'))
  model.add(Dropout(0.2))
  model.add(Dense(128, activation='relu')) 
  model.add(Dense(6, activation='softmax'))
  model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])
  # summarize layers
  model.summary()
  # plot graph
  plot_model(model, to_file='artificial_neural_network.png')
  return model

clean_corpus = []

for line in corpus_:
  line = re.sub(r'http\S+', '', line)
  line = re.sub(r'@\S+', '', line)
  line = deEmojify(line)
  line = clean_doc(line)
  line = [w for w in line if w in vocab] 
  clean_corpus.append(line)

X_train, X_test, y_train, y_test = train_test_split(clean_corpus, emotions_, test_size = 0.1, random_state = 42)

train_docs = X_train
test_docs = X_test

#Fit model

# create the tokenizer
tokenizer = create_tokenizer(train_docs)
# define vocabulary size
vocab_size = len(tokenizer.word_index) + 1
print('Vocabulary size: %d' % vocab_size)
# calculate the maximum sequence length
max_length = max([len(s.split()) for s in train_docs])

print('Maximum length: %d' % max_length)

# encode data
Xtrain = encode_docs(tokenizer, max_length, X_train)
#ytrain = encode_docs(tokenizer_emotions, 6, train_emo)
Xtest = encode_docs(tokenizer, max_length, X_test)
#encode emotions
ytrain=pd.get_dummies(y_train)
ytest=pd.get_dummies(y_test)

# define model
#model = define_ann_model(Xtrain.shape[1])
#model = define_rnn_model(vocab_size, max_length)
model = define_cnn_model(vocab_size, max_length)

# fit network
model.fit(Xtrain, ytrain, epochs=10, verbose=2) #Xtrain yerine X_train koydum
model.save('modelv7.h5')

#Model Evaluation
import string
import re
from keras.models import load_model

# load the model
model = load_model('/content/modelv9.h5')
# evaluate model on training dataset
_, acc = model.evaluate(Xtrain, ytrain, verbose=0)
print('Train Accuracy: %.2f' % (acc*100))
# evaluate model on test dataset

_, acc = model.evaluate(Xtest, ytest, verbose=0)
print('Test Accuracy: %.2f' % (acc*100))



