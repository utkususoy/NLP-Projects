# -*- coding: utf-8 -*-
"""create_vocab

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mbQIdIlarEW2JQ-NoewBlUZlMxAuFrd3
"""

#Create Dictionary
import string
import re
from os import listdir
from numpy import array
from nltk.tokenize import word_tokenize
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.utils.vis_utils import plot_model
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Embedding
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
import csv
from collections import defaultdict
import pandas as pd
from bs4 import BeautifulSoup
from collections import Counter
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
import nltk 
nltk.download('punkt')
from snowballstemmer import TurkishStemmer

turkStem=TurkishStemmer()

def deEmojify(text):
    regrex_pattern = re.compile("["
                u"\U0001F600-\U0001F64F"  # emoticons
                u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                u"\U0001F680-\U0001F6FF"  # transport & map symbols
                u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                u"\U00002702-\U000027B0"
                u"\U000024C2-\U0001F251"
                u"\U0001f926-\U0001f937"
                u'\U00010000-\U0010ffff'
                u"\u200d"
                u"\u2640-\u2642"
                u"\u2600-\u2B55"
                u"\u23cf"
                u"\u23e9"
                u"\u231a"
                u"\u3030"
                u"\ufe0f"
    "]+", flags=re.UNICODE)

    return regrex_pattern.sub(r'',text)

# turn a doc into clean tokens
def clean_doc(doc, tr_stopwords):
  # split into tokens by white space
  tokens = doc.split()
  tokens = [word.lower() for word in tokens] 
  # prepare regex for char filtering
  re_punc = re.compile('[%s]' % re.escape(string.punctuation))
  # remove punctuation from each word
  tokens = [re_punc.sub('', w) for w in tokens]
  # filter out short tokens
  tokens = [turkStem.stemWord(word) for word in tokens]
  tokens = [w for w in tokens if not w in tr_stopwords]
  tokens = [word for word in tokens if len(word) > 1]
  return tokens

def save_list(lines, filename):
  # convert lines to a single blob of text
  data = '\n'.join(lines)
  # open file
  file = open(filename, 'w', encoding='utf-8')
  # write text
  file.write(data)
  # close file
  file.close()

def load_doc_pandas(filename): 
  df = pd.read_excel(filename) # can also index sheet by name or fetch all sheets
  corpus = df['tw_text'].tolist()
  emotions = df['emotion'].tolist()
  id = df['id'].tolist()
  return corpus, emotions, id

def load_file(filename):
  # open the file as read only
  file = open(filename, 'r')
  # read all text
  text = file.read()
  # close the file
  file.close()
  text = word_tokenize(text)
  return text

def stem_stopwords(tr_stopwords):
  #stem's of each word 
  print("cccc",tr_stopwords[0])
  stemed_stopwords = [] 
  for w in tr_stopwords:  
    x = turkStem.stemWord(w) 
    stemed_stopwords.append(x) 
  return stemed_stopwords

stopwords_file = '/content/gdrive/MyDrive/EmotionAnalysis/stopwords_extend.txt' 
tr_stopwords = load_file(stopwords_file)

vocab = Counter()
filename = '/content/gdrive/MyDrive/EmotionAnalysis/Corpurav2.xlsx'

corpus_, emotions_, id_ = load_doc_pandas(filename)

#clean corpura and create vocab
for line in corpus_:
  line = re.sub(r'http\S+', '', line)
  line = re.sub(r'@\S+', '', line)
  line = deEmojify(line)
  line = clean_doc(line, tr_stopwords)
  vocab.update(line)

print(len(vocab))
print(vocab.most_common(50))

min_occurane = 2
final_vocab = [k for k,c in vocab.items() if c >= min_occurane]
print(len(final_vocab))

# save tokens to a vocabulary file
save_list(final_vocab, 'emotional_vocab.txt')